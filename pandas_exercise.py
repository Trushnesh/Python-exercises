# -*- coding: utf-8 -*-
"""Copy of pandas-exercise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1apbqJAZ_-4A_nMPgifc5EgL01VeEH3a0

## Pandas Exercise

### Part 1: Aggregation, Group By and Sort Review

Below is a dataframe, with values as a list-of-lists and columns as a list.
"""

import pandas as pd

df = pd.DataFrame([[123,'xt23',20],
                   [123,'q45',2],
                   [123,'a89',25],
                   [77,'q45',3],
                   [77,'a89',30],
                   [92,'xt23',24],
                   [92,'m33',60],
                   [92,'a89',28]], columns=['userid','product','price'])
df

"""**Q: We want the maximum price anyone has paid.**"""

max(df.price)

"""**Q: We want the maximum price per user. Hint: Use groupby.**"""

df.groupby(['price']).max()

"""**Q: What is the total amount paid by user?**

Note: Pandas will smartly leave out columns for which that aggregation doesn't have meaning.
"""

sum(df.price)

"""**Q: Sort the dataframe by userid first, and then price, both ascending.**"""

df1=df.sort_values(['userid'], ascending=True)
df2=df1.sort_values(['price'], ascending=True)
df2

"""### Part 2: Combining Techniques

**Q: We want the maximum price each user paid, and the product associated with that price.**

Note for SQL users: In SQL, you groupby and then sort, but in pandas, it's easier to do it the other way around.
"""

df.sort_values(['price'], ascending=False)

df_grouped = df.sort_values(
    'price', ascending=False).groupby(['userid'], as_index=False).max()
df_grouped
#Product are displayed incorrectly as per userid and price

"""### Part 3: Exploring The Index

Let's add a new column.
"""

df['website'] = ['Amazon', 'Amazon', 'NewEgg', 'NewEgg',
                 'NewEgg', 'Amazon', 'Amazon', 'Amazon']
df

"""**Q: What is the total amount paid by each user on each website?**"""

df.groupby(['userid', 'website'])[['price']].sum()

"""Now let's do the same groupby as above, but set the as_index flag to "False". This will result in a flat table instead of the nested indexes."""

df.groupby(['userid', 'website'],as_index=False)[['price']].sum()

"""### Part 4: Merging

Let's now create a second table:
"""

df2 = pd.DataFrame([
    [123, 'USA'], [77, 'Canada'], [92, 'USA']],
    columns=['userid', 'country'])
df2

"""We can combine the two tables using a merge function. What it does is, it will do a pairwise comparision of every row in table1 with every row in table2 and if the "on" condition matches, it will create a single row with columns from both those matched rows.

Merge of two tables with 5 rows each can give as little as 0 rows and as much as 25 rows.

    [1,2,3,4,5] merged with [6,7,8,9,10] will give 0 rows
    [1,2,3,4,5] merged with [1,2,3,4,5] will give 5 rows
    [1,1,1,1,1] merged with [1,1,1,1,1] will give 25 rows
"""

df3=pd.merge(df, df2, on='userid')
df3

"""**Q: What is the total amount paid per country?**"""

df3.groupby(['country'],as_index=False)[['price']].sum()

"""**Q: What is the average amount paid per country and website?**"""

df3.groupby(['country','website'])[['price']].mean()

"""### Part 5: The Final Question - A Demo

Let's add another column: purchase date
"""

df['date'] = ['2018-01-12',
              '2018-01-08',
              '2018-01-06',
              '2018-01-03',
              '2018-01-05',
              '2018-01-04',
              '2018-01-07',
              '2018-01-02']
df

"""**Q: Here is a tricky task. For each row, I want the average purchase price for that user prior to that purchase.**

One option is to do some loops. But another solution is to just do a merge on itself and filter.
"""

dff=pd.merge(df,df,on="userid")
dff1=dff[['userid','price_x','date_x']]
dff1

dff2=dff[['userid','price_y','date_y']]
dff2

dff3=dff[dff1['date_x']>dff2['date_y']]
dff3

"""### Movie / Wine Example"""





"""But first, quick question:

Say you merge (or 'join' if you come from SQL) two dataframes with 3 rows each, how many rows would you end up with?

Could be anything between 0-9.

Consider the following examples, where table x has users and the movies they like. And table y has users and the wines they line. And let's do a merge to come up with possible movie and wine pairings for any user. In case A, we get 0 rows, in case B, we get 3 rows and case C we get 9 rows.

#### Merge Two Tables with No IDs in Common
"""

dfx = pd.DataFrame([[1,'Godfather'],[2,'Amelie'],[3,'Chicago']],columns=['userid','movies'])
dfx

dfy = pd.DataFrame([[4,'red'],[5,'white'],[6,'pink']],columns=['userid','wines'])
dfy

dfm = pd.merge(dfx,dfy,on='userid')
dfm

"""#### Merge Two Tables with IDs in Common (1 Value Per ID)"""

dfx = pd.DataFrame([[1,'Godfather'],[2,'Amelie'],[3,'Chicago']],columns=['userid','movies'])
dfx

dfy = pd.DataFrame([[1,'red'],[2,'white'],[3,'pink']],columns=['userid','wines'])
dfy

dfm = pd.merge(dfx,dfy,on='userid')
dfm

"""#### Merge Two Tables with IDs in Common (Multiple Values Per ID)"""

dfx = pd.DataFrame([[1,'Godfather'],[1,'Amelie'],[1,'Chicago']],columns=['userid','movies'])
dfx

dfy = pd.DataFrame([[1,'red'],[1,'white'],[1,'pink']],columns=['userid','wines'])
dfy

dfm = pd.merge(dfx,dfy,on='userid')
dfm

"""### Back to the Question

Now let's return to the original question: For each row, I want the average purchase price for that user prior to that purchase. Let's do a merge on itself and filter.

Here are the steps we're going to take:
1. **MERGE**: Join table on itself. For each userid / date combo, show me all userid / date / price combos.
1. **FILTER**: For each userid / date combo, keep only the userid / date / price combos that were from earlier. Filter everything else out.
1. **AGGREGATE**: For each userid / date combo, find the average price for the remaining rows.
1. **MERGE**: Combine these values with the original dataframe for the final result.
1. **SORT**: Sort to make the results look pretty.

#### MERGE: Join table on itself. For each userid / date combo, show me all userid / date / price combos.
"""

df_date = df[['userid','date']]
df_date

df_all = df[['userid','price','date']]
df_all

df2 = pd.merge(df_date, df_all, on='userid')
df2

"""#### FILTER: For each userid / date combo, keep only the userid / date / price combos that were from earlier. Filter everything else out."""

df3 = df2[df2['date_x'] > df2['date_y']]
df3

"""#### **AGGREGATE**: For each userid / date combo, find the average price for the remaining rows."""

df4 = df3.groupby(['userid','date_x'])[['price']].mean()
df4.rename(columns={'price': 'avg_prior_price'}, inplace=True)
df4

"""#### **MERGE**: Combine these values with the original dataframe for the final result."""

df

df4.index

final = df.merge(df4,
                 left_on=['userid','date'],
                 right_on=['userid','date_x'],
                 right_index=True,
                 how='left')
final

"""#### **SORT**: Sort to make the results look pretty."""

final.sort_values(by=['userid','date'])

